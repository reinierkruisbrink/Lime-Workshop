{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Explain Your Model's Decisions With LIME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "\n",
    "- 1\t[Introduction to LIME](#1)\n",
    "- 2\t[Intuition behind LIME](#2)\n",
    "- 3\t[Python implementation of model development](#3)\n",
    "- 4\t[Interpret model predictions with LIME](#4)\n",
    "    - 4.1 [Import LIME package](#4.1)\n",
    "    - 4.2 [Create the explainer](#4.2)\n",
    "    - 4.3 [Use the explainer to explain predictions](#4.3)\n",
    "- 5 [References](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **1. Introduction to LIME** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) stands for **Local Interpretable Model-agnostic Explanations**. LIME focuses on training local surrogate models to explain individual predictions. Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models.\n",
    "\n",
    "\n",
    "- LIME is model-agnostic, meaning that it can be applied to any machine learning model. The technique attempts to understand the model by perturbing the input of data samples and understanding how the predictions change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![LIME](https://miro.medium.com/max/1165/1*k-rxjnvUDTwk8Jfg6IYBkQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model-specific approaches aim to understand the black model machine learning model by analysing the internal components and how they interact. LIME provides local model interpretability. LIME modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. The most common question is probably: why was this prediction made or which variables caused the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **2. Intuition behind LIME** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- The intuition behind LIME is very simple. First, forget the training data and imagine we have only the black box model where we supply the input data. The black box model generate the predictions for the model. We can enquire the box as many times as we like. Our objective is to understand why the machine learning model made a certain prediction. \n",
    "\n",
    "- Now, [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) comes into play. LIME tests what happens to the predictions when we provide variations in the data which is being fed into the machine learning model. \n",
    "\n",
    "- [LIME](https://christophm.github.io/interpretable-ml-book/lime.html) generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an [interpretable model](https://christophm.github.io/interpretable-ml-book/simple.html#simple). It is weighted by the proximity of the sampled instances to the instance of interest. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called local fidelity.\n",
    "\n",
    "- A global decision boundary is likely to be non linear but if you zoom in enough, it probably is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mathematically, local surrogate models with interpretability constraint can be expressed as follows:\n",
    "\n",
    "$$explanation(x)=arg min_g∈G L(f,g,πx)+Ω(g)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The explanation model for instance x is the model g (e.g. linear regression model) that minimizes loss function L (e.g. mean squared error). It measures how close the explanation is to the prediction of the original model f (e.g. an xgboost model), while the model complexity Ω(g) is kept low (e.g. prefer fewer features). G is the family of possible explanations. \n",
    "\n",
    "- In practice, LIME only optimizes the loss part. The user has to determine the complexity, e.g. by selecting the maximum number of features that the linear regression model may use.\n",
    "\n",
    "- So, the recipe for training local surrogate models is as follows:\n",
    "\n",
    "  - 1 Select your instance of interest for which you want to have an explanation of its black box prediction.\n",
    "  - 2 Perturb your dataset and get the black box predictions for these new points.\n",
    "  - 3 Weight the new samples according to their proximity to the instance of interest.\n",
    "  - 4 Train a weighted, interpretable model on the dataset with the variations.\n",
    "  - 5 Explain the prediction by interpreting the local model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **3. Model development** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Load Preliminaries** <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Read Data** <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we will aim to predict energy ratings of building in New York City. This dataset contains 41 features of which some are probably not that useful for predicting these energy ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read and preview data\n",
    "df = pd.read_csv(\"data/NYC_Energy_Water_Data.csv\").replace(\n",
    "    {\"Not Available\": np.nan, \"Not found\": np.nan}\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3.3 View Summary of data** <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below to see a description of every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description = pd.read_csv('data/Description.csv')\n",
    "# for i in range(len(description)):\n",
    "#     print(description.loc[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are all the variables which we will use:\n",
    "\n",
    "  - 1 `Largest Property Use Type - Gross Floor Area (ft²)` - Floor area, there can be multiple property types (hotel/office/...) per building, we only consider the largest one.\n",
    "  - 2 `Year Built` - This is the year in which your property was constructed. If your property has undergone a complete renovation that included gutting and rebuilding the interior, then you can indicate the date of this renovation as the year built.\n",
    "  - 3 `Occupancy` - The percentage of your property’s Gross Floor Area (GFA) that is occupied and operational.\n",
    "  - 4 `Weather Normalized Site EUI (kBtu/ft²)` - Energy use intensity as calculated at the property site in kBtus per gross square foot (kBtu/ft2) for the reporting year, normalized for weather.\n",
    "  - 5 `Weather Normalized Site Electricity Intensity (kWh/ft²)` - Weather Normalized Site Energy divided by property size or by flow through a water/wastewater treatment plant.\n",
    "  - 6 `Weather Normalized Site Natural Gas Intensity (therms/ft²)` - Weather Normalized Site Energy divided by property size or by flow through a water/wastewater treatment plant.\n",
    "  - 7 `Weather Normalized Site Natural Gas Use (therms)` - The energy use your property would have consumed during 30-year average weather conditions\n",
    "  - 8 `Electricity Use - Grid Purchase (kBtu)` - Energy Use by Type is a summary of the annual consumption of an individual type of energy. Annual totals are available for Electricity Use - Grid Purchase.\n",
    "  - 9 `Weather Normalized Site Electricity (kWh)` - The energy use your property would have consumed during 30-year average weather conditions\n",
    "  - 10 `Total GHG Emissions (Metric Tons CO2e)` - The total direct and indirect greenhouse gases emitted by the property, reported in metric tons of carbon dioxide equivalent (MtCO2e) for the reporting year.\n",
    "  - 11 `Direct GHG Emissions (Metric Tons CO2e)` - The total direct greenhouse gases emitted by the property, reported in metric tons of carbon dioxide equivalent (MtCO2e) for the reporting year.\n",
    "  - 12 `Indirect GHG Emissions (Metric Tons CO2e)` - The total indirect greenhouse gases emitted by the property, reported in metric tons of carbon dioxide equivalent (MtCO2e) for the reporting year.\n",
    "  - 13 `Water Use (All Water Sources) (kgal)` - Sum of all water meters.\n",
    "\n",
    "\n",
    "- The target variable is `ENERGY STAR Score`, which is a 1-to-100 percentile ranking for specified building types. A high rating is good and low is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows where we have no target variable because there is nothing to predict\n",
    "df = df.dropna(subset=[\"ENERGY STAR Score\"])\n",
    "\n",
    "# Only select numerical features (see the text cell above for descriptions).\n",
    "relevant_features = [\n",
    "    \"Largest Property Use Type - Gross Floor Area (ft²)\",\n",
    "    \"Year Built\",\n",
    "    \"Occupancy\",\n",
    "    \"ENERGY STAR Score\",\n",
    "    \"Weather Normalized Site EUI (kBtu/ft²)\",\n",
    "    \"Weather Normalized Site Electricity Intensity (kWh/ft²)\",\n",
    "    \"Weather Normalized Site Natural Gas Intensity (therms/ft²)\",\n",
    "    \"Weather Normalized Site Natural Gas Use (therms)\",\n",
    "    \"Electricity Use - Grid Purchase (kBtu)\",\n",
    "    \"Weather Normalized Site Electricity (kWh)\",\n",
    "    \"Total GHG Emissions (Metric Tons CO2e)\",\n",
    "    \"Direct GHG Emissions (Metric Tons CO2e)\",\n",
    "    \"Indirect GHG Emissions (Metric Tons CO2e)\",\n",
    "    \"Water Use (All Water Sources) (kgal)\",\n",
    "]\n",
    "df = df[relevant_features].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Missing values treatment** <a class=\"anchor\" id=\"3.4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are quite a lot of missing values in the dataset. For convenience, we will fill them by the mean of respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that there are no missing values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 Feature Vector and Target Variable** <a class=\"anchor\" id=\"3.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the target variable from the features\n",
    "relevant_features.remove(\"ENERGY STAR Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare feature vector and target variable\n",
    "X = df[relevant_features]\n",
    "y = df[\"ENERGY STAR Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 Train-Test Split** <a class=\"anchor\" id=\"3.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test data:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.7 Build the Random Forest model** <a class=\"anchor\" id=\"3.7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the model with Random Forest Regressor :\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.8 Generate Predictions** <a class=\"anchor\" id=\"3.8\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.9 Evaluate Performance** <a class=\"anchor\" id=\"3.9\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, y_pred) ** (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **4. Interpret model predictions with LIME** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Import LIME package** <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Create the Explainer** <a class=\"anchor\" id=\"4.2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LIME has one explainer for all the models\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    feature_names=X_train.columns.values.tolist(),\n",
    "    class_names=[\"ENERGY STAR Score\"],\n",
    "    verbose=True,\n",
    "    mode=\"regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Use the explainer to explain predictions** <a class=\"anchor\" id=\"4.3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we will choose 5 instances and use them to explain the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Select 3rd instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the 3rd instance and use it to predict the results\n",
    "j = 3\n",
    "# Use just the first 5 features for this example for illustrative purposes\n",
    "exp = explainer.explain_instance(X_test.values[j], model.predict, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The true value of this instance\n",
    "y_test.iloc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the predictions\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This list shows the decision rules and the impact it has on the outcome, see the values in the table on the right above here.\n",
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "- The predicted value of the energy star score is 17.90 and the true score is 21.\n",
    "- The prediction is mostly driven by the `Weather Normalized Site Electricity Intensity`, and in this case negatively.\n",
    "- `Weather Normalized Site EUI` and `Gross Floor Area` also negatively impact the model's decision.\n",
    "- `Total GHG Emissions` and `Weather Normalized Site Natural Gas Intensity` impact the prediction positively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Select 11th instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the 10th instance and use it to predict the results\n",
    "j = 11\n",
    "# This time we use 10 features\n",
    "exp = explainer.explain_instance(X_test.values[j], model.predict, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The true value of this instance\n",
    "y_test.iloc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the predictions\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "- The predicted value of the energy star score is 98.60 and the true score is 100.\n",
    "- The prediction is again most strongly driven by the `Weather Normalized Site EUI`, but positively in this case.\n",
    "- For example, the second most important feature: `Total GHG Emissions` is below the threshold of 261.5 (see the list) and therefore is an indicator of a well performing building in terms of energy usage. In the table (right top) the corresponding value of 42.90 is in fact lower than that threshold in the list and so we can see the reasons that our model bases its very high predicted `ENERGY STAR Score` on.\n",
    "- (Remember that LIME uses local thresholds so the decision rules do not apply for every instance! Intuitively, a certain amount of gas use is not necessarily good or bad, it depends on how big the building is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Select 16th instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the 16th instance and use it to predict the results\n",
    "j = 16\n",
    "# Use all features.\n",
    "exp = explainer.explain_instance(\n",
    "    X_test.values[j], model.predict, num_features=len(relevant_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.iloc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the predictions\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "- The predicted value of the energy star score is 34.4 and the true score is 50.\n",
    "- In this case we use all 13 features but we clearly see that most at the bottom have little impact on the prediction. In fact, `Occupancy` has no impact at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Interpret an instance DIY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select a random instance and use it to predict the results\n",
    "j = np.random.randint(len(X_test), size=1).item()\n",
    "print(j)\n",
    "# Pick an amount of features, you can always change it an run it again\n",
    "n1 = \n",
    "exp = explainer.explain_instance(X_test.values[j], model.predict, num_features=n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Energy rating\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the predictions\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature threhold and rules\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "- [Fill this in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Interpret another instance DIY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select a random instance and use it to predict the results\n",
    "j = np.random.randint(len(X_test), size=1).item()\n",
    "print(j)\n",
    "# Pick an amount of features, you can always change it an run it again\n",
    "n2 = \n",
    "exp = explainer.explain_instance(X_test.values[j], model.predict, num_features=n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Energy rating\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictions\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature threhold and rules\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "\n",
    "- [Fill this in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some question to think about:\n",
    "- 1. What is the most important feature in most (if not all) cases and why? (hint: EUI stands for Energy use intensity)\n",
    "- 2. What is the impact of the amount of features used in LIME?\n",
    "- 3. Can you generalize about your model's decision with LIME? Why or why not?\n",
    "- 4. Does LIME only inform you about your model or might you learn something about the data and the underlying problem as well?\n",
    "\n",
    "Write your answers down or think about them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "tags": []
   },
   "source": [
    "# **5. References** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "The work done in this kernel is based on the following resources:\n",
    "\n",
    "- 1 https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime\n",
    "- 2 https://www.kaggle.com/datasets/mikhailma/energy-efficiency-of-buildings-in-new-york\n",
    "- 3 https://christophm.github.io/interpretable-ml-book/\n",
    "- 4 https://christophm.github.io/interpretable-ml-book/lime.html\n",
    "- 5 https://blog.dominodatalab.com/shap-lime-python-libraries-part-2-using-shap-lime/\n",
    "- 6 https://www.analyticsvidhya.com/blog/2017/06/building-trust-in-machine-learning-models/\n",
    "- 7 https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b\n",
    "- 8 https://marcotcr.github.io/lime/tutorials/Using%2Blime%2Bfor%2Bregression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
